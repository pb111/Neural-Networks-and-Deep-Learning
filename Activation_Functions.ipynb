{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Activation Functions",
      "provenance": [],
      "authorship_tag": "ABX9TyO5OR/KD28bS/BfSrq873Hu",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/pb111/Neural-Networks-and-Deep-Learning/blob/main/Activation_Functions.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JxNUvf5X5zAM"
      },
      "source": [
        "# **[Activation Functions](https://ml-cheatsheet.readthedocs.io/en/latest/activation_functions.html)**\n",
        "\n",
        "\n",
        "- In this notebook, we will discuss **Activation Functions** in neural nets."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kicfvk_ZSojI"
      },
      "source": [
        "## **Table of Contents**\n",
        "\n",
        "- 1  Activation Functions\n",
        "\n",
        "- 2  Types of Activation Functions\n",
        "\n",
        "  - [1. Linear](https://ml-cheatsheet.readthedocs.io/en/latest/activation_functions.html#linear)\n",
        "  - [2. ELU](https://ml-cheatsheet.readthedocs.io/en/latest/activation_functions.html#elu)\n",
        "  - [3. ReLU](https://ml-cheatsheet.readthedocs.io/en/latest/activation_functions.html#relu)\n",
        "  - [4. LeakyReLU](https://ml-cheatsheet.readthedocs.io/en/latest/activation_functions.html#leakyrelu)\n",
        "  - [5. Sigmoid](https://ml-cheatsheet.readthedocs.io/en/latest/activation_functions.html#sigmoid)\n",
        "  - [6. Tanh](https://ml-cheatsheet.readthedocs.io/en/latest/activation_functions.html#tanh)\n",
        "  - [7. Softmax](https://ml-cheatsheet.readthedocs.io/en/latest/activation_functions.html#softmax)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ut5i_Fzz546C"
      },
      "source": [
        "## **[1. Activation Functions](https://ml-cheatsheet.readthedocs.io/en/latest/activation_functions.html)**\n",
        "\n",
        "\n",
        "- **Activation functions** live inside neural network layers and modify the data they receive before passing it to the next layer. \n",
        "- **Activation functions** give neural networks their power — allowing them to model complex non-linear relationships. \n",
        "- By modifying inputs with non-linear functions neural networks can model highly complex relationships between features. \n",
        "- Popular activation functions include [relu](https://ml-cheatsheet.readthedocs.io/en/latest/activation_functions.html#activation-relu) and [sigmoid](https://ml-cheatsheet.readthedocs.io/en/latest/activation_functions.html#activation-sigmoid).\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GtK3X8rZVr4X"
      },
      "source": [
        "Activation functions typically have the following properties:\n",
        "\n",
        "- **Non-linear** - In linear regression we’re limited to a prediction equation that looks like a straight line. This is nice for simple datasets with a one-to-one relationship between inputs and outputs, but what if the patterns in our dataset were non-linear? (e.g. x2, sin, log). To model these relationships we need a non-linear prediction equation. Activation functions provide this non-linearity.\n",
        "\n",
        "- **Continuously differentiable** — To improve our model with gradient descent, we need our output to have a nice slope so we can compute error derivatives with respect to weights. If our neuron instead outputted 0 or 1 (perceptron), we wouldn’t know in which direction to update our weights to reduce our error.\n",
        "\n",
        "- **Fixed Range** — Activation functions typically squash the input data into a narrow range that makes training the model more stable and efficient."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7BhKszjj9ARD"
      },
      "source": [
        "## **2. Types of Neural Network Activation Functions**\n",
        "\n",
        "- Here, we will discuss various types of **Neural Network Activation Functions**. There are different types of activation functions which are listed below-\n",
        "\n",
        "- 1  Linear\n",
        "- 2  ELU\n",
        "- 3  ReLU\n",
        "- 4  LeakyReLU\n",
        "- 5  Sigmoid\n",
        "- 6  Tanh\n",
        "- 7  Softmax"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DwM0_zFOY7gf"
      },
      "source": [
        "## **[1. Linear](https://ml-cheatsheet.readthedocs.io/en/latest/activation_functions.html#linear)**\n",
        "\n",
        "\n",
        "- A straight line function where activation is proportional to input ( which is the weighted sum from neuron )."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2sUPU52yaIH_"
      },
      "source": [
        "Function\n",
        "\n",
        "       - R(z,m)={z∗m}\n",
        "\n",
        "  ![Linear function](https://ml-cheatsheet.readthedocs.io/en/latest/_images/linear.png)\n",
        "\n",
        "\n",
        "   "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-E1uDRbab8GK"
      },
      "source": [
        "def linear(z,m):\n",
        "    return m*z\n"
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lBLwitQWcNrJ"
      },
      "source": [
        "Derivative\n",
        "\n",
        "    - R′(z,m)={m}\n",
        "\n",
        "![Derivative of Linear Function](https://ml-cheatsheet.readthedocs.io/en/latest/_images/linear_prime.png)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wEM6PaPGclr2"
      },
      "source": [
        "def linear_prime(z,m):\n",
        "    return m"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7YT5ZOg_dwTR"
      },
      "source": [
        "#### **Pros**\n",
        "\n",
        "- It gives a range of activations, so it is not binary activation.\n",
        "- We can definitely connect a few neurons together and if more than 1 fires, we could take the max ( or softmax) and decide based on that.\n",
        "\n",
        "\n",
        "#### **Cons**\n",
        "\n",
        "- For this function, derivative is a constant. That means, the gradient has no relationship with X.\n",
        "- It is a constant gradient and the descent is going to be on constant gradient.\n",
        "- If there is an error in prediction, the changes made by back propagation is constant and not depending on the change in input delta(x) !"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8Xajqldie1j2"
      },
      "source": [
        "## **[2. ELU](https://ml-cheatsheet.readthedocs.io/en/latest/activation_functions.html#elu)**\n",
        "\n",
        "\n",
        "- **Exponential Linear Unit** or its widely known name **ELU** is a function that tend to converge cost to zero faster and produce more accurate results. \n",
        "- Different to other activation functions, ELU has a extra alpha constant which should be positive number."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "G7UN-uJHhPaY"
      },
      "source": [
        "- ELU is very similiar to RELU except negative inputs. They are both in identity function form for non-negative inputs. \n",
        "- On the other hand, ELU becomes smooth slowly until its output equal to -α whereas RELU sharply smoothes."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GY62Opx0kZ6S"
      },
      "source": [
        "Function\n",
        "\n",
        "     - R(z) = {  z   ,  z > 0\n",
        "              α.(e^z–1)  z<= 0 }\n",
        "\n",
        "\n",
        "![ELU function](https://ml-cheatsheet.readthedocs.io/en/latest/_images/elu.png)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_sP3S7-OpXNy"
      },
      "source": [
        "def elu(z,alpha):\n",
        "\treturn z if z >= 0 else alpha*(e^z -1)"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HpXbkm1UpsNO"
      },
      "source": [
        "Derivative\n",
        "\n",
        "     - R′(z) = {1  ,  z > 0\n",
        "               α.e^z  z < 0}\n",
        "\n",
        "![ELU Prime Function](https://ml-cheatsheet.readthedocs.io/en/latest/_images/elu_prime.png)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "glxzvuhlqoEZ"
      },
      "source": [
        "def elu_prime(z,alpha):\n",
        "\treturn 1 if z > 0 else alpha*np.exp(z)"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OKDq6DLc9VUe"
      },
      "source": [
        "#### **Pros**\n",
        "\n",
        "- ELU becomes smooth slowly until its output equal to -α whereas RELU sharply smoothes.\n",
        "- ELU is a strong alternative to ReLU.\n",
        "- Unlike to ReLU, ELU can produce negative outputs.\n",
        "\n",
        "#### **Cons**\n",
        "\n",
        "- For x > 0, it can blow up the activation with the output range of [0, inf]."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BG5Ajjks9vEP"
      },
      "source": [
        "## **[3. ReLU](https://ml-cheatsheet.readthedocs.io/en/latest/activation_functions.html#relu)**\n",
        "\n",
        "\n",
        "- A recent invention which stands for **Rectified Linear Units**. \n",
        "- The formula is deceptively simple : **max(0,z)**. \n",
        "- Despite its name and appearance, it’s not linear and provides the same benefits as Sigmoid but with better performance."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "P0F_ZE-UBp28"
      },
      "source": [
        "Function\n",
        "\n",
        "      - R(z) = {z  z > 0\n",
        "                0  z <= 0}\n",
        "\n",
        "![ReLU Function](https://ml-cheatsheet.readthedocs.io/en/latest/_images/relu.png)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2J7da1TJC_Ue"
      },
      "source": [
        "def relu(z):\n",
        "  return max(0, z)"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PfJ6zZzXDJa8"
      },
      "source": [
        "Derivative\n",
        "\n",
        "\n",
        "     - R′(z) = {1  z > 0\n",
        "                0  z < 0}"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uoNb-kf4E3we"
      },
      "source": [
        "![ReLU Prime Function](https://ml-cheatsheet.readthedocs.io/en/latest/_images/relu_prime.png)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MeFJy3VUDFI3"
      },
      "source": [
        "def relu_prime(z):\n",
        "  return 1 if z > 0 else 0"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "c8CFV4V-GL6V"
      },
      "source": [
        "#### **Pros**\n",
        "\n",
        "- It avoids and rectifies **vanishing gradient problem**.\n",
        "- **ReLu** is less computationally expensive than tanh and sigmoid because it involves simpler mathematical operations.\n",
        "\n",
        "\n",
        "#### **Cons**\n",
        "\n",
        "- One of its limitation is that it should only be used within Hidden layers of a Neural Network Model.\n",
        "- Some gradients can be fragile during training and can die. It can cause a weight update which will makes it never activate on any data point again. Simply saying that ReLu could result in Dead Neurons.\n",
        "- In another words, for activations in the region (x < 0) of ReLu, gradient will be 0 because of which the weights will not get adjusted during descent. That means, those neurons which go into that state will stop responding to variations in error/ input ( simply because gradient is 0, nothing changes ). This is called **dying ReLu problem**.\n",
        "- The range of ReLu is [0, inf). This means it can blow up the activation."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jDgWJ0OoH0Fo"
      },
      "source": [
        "## **[4. LeakyReLU](https://ml-cheatsheet.readthedocs.io/en/latest/activation_functions.html#leakyrelu)**\n",
        "\n",
        "- **LeakyRelu** is a variant of ReLU. \n",
        "- Instead of being 0 when z < 0, a leaky ReLU allows a small, non-zero, constant gradient α (Normally, α=0.01). \n",
        "- However, the consistency of the benefit across tasks is presently unclear. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "q9UYnOh5I_y5"
      },
      "source": [
        "Function\t\n",
        "\n",
        "    - R(z) = {z  z > 0\n",
        "              αz   z <= 0}\n",
        "\n",
        "\n",
        "![LeakyReLU Function](https://ml-cheatsheet.readthedocs.io/en/latest/_images/leakyrelu.png)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vo0w0_guHVUr"
      },
      "source": [
        "def leakyrelu(z, alpha):\n",
        "\treturn max(alpha * z, z)"
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7hwRTHxWJylh"
      },
      "source": [
        "Derivative\n",
        "\n",
        "     - R′(z) = {1 z > 0\n",
        "                α z < 0}\n",
        "\n",
        "\n",
        "![LeakyReLU Function](https://ml-cheatsheet.readthedocs.io/en/latest/_images/leakyrelu_prime.png)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-Lmq-QvdJrvm"
      },
      "source": [
        "def leakyrelu_prime(z, alpha):\n",
        "\treturn 1 if z > 0 else alpha"
      ],
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "a9tSeZYGLsSG"
      },
      "source": [
        "#### **Pros**\n",
        "\n",
        "- Leaky ReLUs are one attempt to fix the **“dying ReLU”** problem by having a small negative slope (of 0.01, or so).\n",
        "\n",
        "\n",
        "#### **Cons**\n",
        "\n",
        "- As it possess linearity, it can’t be used for the complex Classification. \n",
        "- It lags behind the [Sigmoid](https://ml-cheatsheet.readthedocs.io/en/latest/activation_functions.html#sigmoid) and [Tanh](https://ml-cheatsheet.readthedocs.io/en/latest/activation_functions.html#tanh) for some of the use cases."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zAQmrqqOM_kt"
      },
      "source": [
        "## **[5. Sigmoid](https://ml-cheatsheet.readthedocs.io/en/latest/activation_functions.html#sigmoid)**\n",
        "\n",
        "\n",
        "- Sigmoid takes a real value as input and outputs another value between 0 and 1. \n",
        "- It’s easy to work with and has all the nice properties of activation functions: it’s \n",
        "   - non-linear \n",
        "   - continuously differentiable \n",
        "   - monotonic and \n",
        "   - has a fixed output range."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "m3XQV78XNxp7"
      },
      "source": [
        "Function\n",
        "\n",
        "\n",
        "    - S(z) = 1 / (1 + e ^ −z)\n",
        "\n",
        "\n",
        "![Sigmoid Function](https://ml-cheatsheet.readthedocs.io/en/latest/_images/sigmoid.png)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mg_NDlxqMB_L"
      },
      "source": [
        "def sigmoid(z):\n",
        "  return 1.0 / (1 + np.exp(-z))"
      ],
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "re0B-2-xPA4V"
      },
      "source": [
        "Derivative\n",
        "\n",
        "    - S′(z) = S(z).(1−S(z))\n",
        "\n",
        "\n",
        "![Sigmoid Prime Function](https://ml-cheatsheet.readthedocs.io/en/latest/_images/sigmoid_prime.png)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "K6FPRyl-O71N"
      },
      "source": [
        "def sigmoid_prime(z):\n",
        "  return sigmoid(z) * (1-sigmoid(z))"
      ],
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "biIplY-XRcWr"
      },
      "source": [
        "#### **Pros**\n",
        "\n",
        "- It is nonlinear in nature. Combinations of this function are also nonlinear!\n",
        "- It will give an analog activation unlike step function.\n",
        "- It has a smooth gradient too.\n",
        "- It’s good for a classifier.\n",
        "- The output of the activation function is always going to be in range (0,1) compared to (-inf, inf) of linear function. So we have our activations bound in a range. Nice, it won’t blow up the activations then.\n",
        "\n",
        "\n",
        "#### **Cons**\n",
        "\n",
        "- Towards either end of the sigmoid function, the Y values tend to respond very less to changes in X.\n",
        "- It gives rise to a problem of **“vanishing gradients”.**\n",
        "- Its output isn’t zero centered. It makes the gradient updates go too far in different directions. 0 < output < 1, and it makes optimization harder.\n",
        "- Sigmoids saturate and kill gradients.\n",
        "- The network refuses to learn further or is drastically slow ( depending on use case and until gradient /computation gets hit by floating point value limits )."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "a9HFl2GDR3Fx"
      },
      "source": [
        "## **[6. Tanh](https://ml-cheatsheet.readthedocs.io/en/latest/activation_functions.html#tanh)**\n",
        "\n",
        "\n",
        "- **Tanh** squashes a real-valued number to the range [-1, 1]. \n",
        "- It’s non-linear. But unlike Sigmoid, its output is zero-centered. \n",
        "- Therefore, in practice the tanh non-linearity is always preferred to the sigmoid nonlinearity. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vhbzBA8OZloG"
      },
      "source": [
        "Function\t\n",
        "\n",
        "    - tanh(z) = (e^z − e^−z) / (e^z + e^−z)\n",
        "\n",
        "\n",
        "![tanh function](https://ml-cheatsheet.readthedocs.io/en/latest/_images/tanh.png)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "te3L3WqKaCQx"
      },
      "source": [
        "def tanh(z):\n",
        "\treturn (np.exp(z) - np.exp(-z)) / (np.exp(z) + np.exp(-z))"
      ],
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WyeiBkRvaPV3"
      },
      "source": [
        "Derivative\n",
        "\n",
        "\n",
        "    - tanh′(z) = 1 − tanh(z)^2\n",
        "\n",
        "\n",
        "![tanh prime function](https://ml-cheatsheet.readthedocs.io/en/latest/_images/tanh_prime.png)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8BpnlHK9aiM4"
      },
      "source": [
        "def tanh_prime(z):\n",
        "\treturn 1 - np.power(tanh(z), 2)"
      ],
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "e2NVbz2saqPR"
      },
      "source": [
        "#### **Pros**\n",
        "\n",
        "- The gradient is stronger for tanh than sigmoid ( derivatives are steeper).\n",
        "\n",
        "\n",
        "#### **Cons**\n",
        "\n",
        "- Tanh also has the **vanishing gradient problem**."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "96-YGUx_Wa-A"
      },
      "source": [
        "## **[7. Softmax](https://ml-cheatsheet.readthedocs.io/en/latest/activation_functions.html#softmax)**\n",
        "\n",
        "\n",
        "- Softmax function calculates the probabilities distribution of the event over ‘n’ different events. \n",
        "- In general way of saying, this function will calculate the probabilities of each target class over all possible target classes. \n",
        "- Later the calculated probabilities will be helpful for determining the target class for the given inputs."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mBSRWXAWW_ZY"
      },
      "source": [
        "Ref : https://ml-cheatsheet.readthedocs.io/en/latest/activation_functions.html"
      ]
    }
  ]
}